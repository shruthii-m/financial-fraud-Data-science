{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5df0de",
   "metadata": {},
   "source": [
    "# Financial Fraud Detection - Exploratory Data Analysis\n",
    "\n",
    "This notebook provides a comprehensive exploratory data analysis of the Financial Fraud Detection dataset by Aman Ali Siddiqui from Kaggle. We'll examine transaction patterns, fraud distribution, and key insights to guide our modeling approach.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Project Structure Setup](#project-structure)\n",
    "2. [Requirements File Creation](#requirements)\n",
    "3. [Data Loading and Initial Exploration](#data-loading)\n",
    "4. [Preprocessing Script Development](#preprocessing)\n",
    "5. [Feature Engineering Implementation](#feature-engineering)\n",
    "6. [Model Development Framework](#modeling)\n",
    "7. [Evaluation Metrics Setup](#evaluation)\n",
    "8. [Streamlit App Foundation](#streamlit-app)\n",
    "9. [Documentation Setup](#documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c612f",
   "metadata": {},
   "source": [
    "## 1. Project Structure Setup {#project-structure}\n",
    "\n",
    "Let's start by creating the complete folder structure for our fraud detection project using Python's pathlib module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f678d8b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for project setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2725df1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create project directory structure\n",
    "project_root = Path(\"../\")  # Parent directory of notebooks folder\n",
    "directories = [\n",
    "    \"data/raw\",\n",
    "    \"data/processed\", \n",
    "    \"data/interim\",\n",
    "    \"notebooks\",\n",
    "    \"src\",\n",
    "    \"streamlit_app\",\n",
    "    \"models\",\n",
    "    \"reports/figures\",\n",
    "    \"reports/results\",\n",
    "    \"config\"\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in directories:\n",
    "    dir_path = project_root / directory\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"✓ Created/verified directory: {directory}\")\n",
    "\n",
    "# Display project structure\n",
    "print(\"\\n Project Structure:\")\n",
    "print(\"financial_fraud_detection/\")\n",
    "for directory in directories:\n",
    "    level = len(directory.split('/')) - 1\n",
    "    indent = \"  \" * level\n",
    "    folder_name = directory.split('/')[-1]\n",
    "    print(f\"{indent}├── {folder_name}/\")\n",
    "\n",
    "print(\"\\n Project structure setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab9898",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Requirements File Creation {#requirements}\n",
    "\n",
    "Let's create a comprehensive requirements.txt file with all necessary libraries for our fraud detection project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e8c21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate requirements.txt content\n",
    "requirements_content = \"\"\"# Core Data Science Libraries\n",
    "pandas>=1.5.0\n",
    "numpy>=1.21.0\n",
    "scikit-learn>=1.1.0\n",
    "scipy>=1.9.0\n",
    "\n",
    "# Machine Learning & Advanced Models\n",
    "xgboost>=1.6.0\n",
    "lightgbm>=3.3.0\n",
    "catboost>=1.0.0\n",
    "imbalanced-learn>=0.9.0\n",
    "\n",
    "# Visualization Libraries\n",
    "matplotlib>=3.5.0\n",
    "seaborn>=0.11.0\n",
    "plotly>=5.10.0\n",
    "\n",
    "# Web Application\n",
    "streamlit>=1.15.0\n",
    "\n",
    "# Jupyter Notebook\n",
    "jupyter>=1.0.0\n",
    "notebook>=6.4.0\n",
    "ipywidgets>=7.7.0\n",
    "\n",
    "# Data Processing & Utils\n",
    "joblib>=1.1.0\n",
    "openpyxl>=3.0.0\n",
    "\n",
    "# Model Interpretation\n",
    "shap>=0.41.0\n",
    "\n",
    "# Progress Bars & Utilities\n",
    "tqdm>=4.64.0\n",
    "\n",
    "# Development & Testing\n",
    "pytest>=7.0.0\n",
    "\n",
    "# Environment Management\n",
    "python-dotenv>=0.19.0\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Write requirements.txt file\n",
    "requirements_path = project_root / \"requirements.txt\"\n",
    "with open(requirements_path, 'w') as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "print(\"✓ requirements.txt file created successfully!\")\n",
    "print(f\" Location: {requirements_path}\")\n",
    "print(\"\\n  Key libraries included:\")\n",
    "print(\"  • pandas, numpy - Data manipulation\")\n",
    "print(\"  • scikit-learn - Machine learning\")\n",
    "print(\"  • xgboost, lightgbm - Advanced ML models\")\n",
    "print(\"  • matplotlib, seaborn, plotly - Visualization\")\n",
    "print(\"  • streamlit - Web application\")\n",
    "print(\"  • jupyter - Interactive notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c232228",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Initial Exploration {#data-loading}\n",
    "\n",
    "Now let's create sample data for demonstration and perform initial exploration. In a real project, you would load the actual Kaggle dataset here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650ebba5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create sample fraud detection dataset (similar to Kaggle dataset structure)\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "\n",
    "# Generate sample data with realistic patterns\n",
    "sample_data = {\n",
    "    'step': np.random.randint(1, 744, n_samples),  # 1 month in hours\n",
    "    'type': np.random.choice(['PAYMENT', 'TRANSFER', 'CASH_OUT', 'DEBIT', 'CASH_IN'], \n",
    "                           n_samples, p=[0.4, 0.2, 0.2, 0.1, 0.1]),\n",
    "    'amount': np.random.lognormal(5, 2, n_samples),\n",
    "    'nameOrig': [f'C{i}' for i in np.random.randint(1, 1000, n_samples)],\n",
    "    'oldbalanceOrg': np.random.lognormal(8, 2, n_samples),\n",
    "    'newbalanceOrig': np.random.lognormal(8, 2, n_samples),\n",
    "    'nameDest': [f'M{i}' for i in np.random.randint(1, 500, n_samples)],\n",
    "    'oldbalanceDest': np.random.lognormal(8, 2, n_samples),\n",
    "    'newbalanceDest': np.random.lognormal(8, 2, n_samples),\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "# Create fraud labels (5% fraud rate)\n",
    "fraud_mask = np.random.choice([0, 1], n_samples, p=[0.95, 0.05])\n",
    "df['isFraud'] = fraud_mask\n",
    "\n",
    "# Make fraud cases more realistic\n",
    "fraud_indices = df[df['isFraud'] == 1].index\n",
    "df.loc[fraud_indices, 'type'] = np.random.choice(['TRANSFER', 'CASH_OUT'], len(fraud_indices))\n",
    "df.loc[fraud_indices, 'amount'] = np.random.lognormal(7, 1.5, len(fraud_indices))\n",
    "\n",
    "print(\"  Sample Dataset Created!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n  First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beafe836",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "print(\"  Dataset Overview:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total transactions: {len(df):,}\")\n",
    "print(f\"Fraud cases: {df['isFraud'].sum():,}\")\n",
    "print(f\"Fraud rate: {(df['isFraud'].sum() / len(df)) * 100:.2f}%\")\n",
    "print(f\"Legitimate transactions: {(df['isFraud'] == 0).sum():,}\")\n",
    "\n",
    "print(\"\\n  Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n  Statistical Summary:\")\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9af9c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize fraud distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Financial Fraud Detection - Data Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Fraud vs Legitimate distribution\n",
    "fraud_counts = df['isFraud'].value_counts()\n",
    "axes[0,0].pie(fraud_counts.values, labels=['Legitimate', 'Fraud'], autopct='%1.1f%%', \n",
    "              colors=['lightblue', 'red'], startangle=90)\n",
    "axes[0,0].set_title('Transaction Distribution')\n",
    "\n",
    "# 2. Transaction types\n",
    "type_counts = df['type'].value_counts()\n",
    "axes[0,1].bar(type_counts.index, type_counts.values, color='skyblue')\n",
    "axes[0,1].set_title('Transaction Types')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Amount distribution by fraud status\n",
    "df_fraud = df[df['isFraud'] == 1]['amount']\n",
    "df_legit = df[df['isFraud'] == 0]['amount']\n",
    "\n",
    "axes[1,0].hist([df_legit, df_fraud], bins=50, alpha=0.7, \n",
    "               label=['Legitimate', 'Fraud'], color=['blue', 'red'])\n",
    "axes[1,0].set_xlabel('Amount')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].set_title('Amount Distribution by Fraud Status')\n",
    "axes[1,0].set_yscale('log')\n",
    "axes[1,0].legend()\n",
    "\n",
    "\n",
    "\n",
    "# 4. Fraud by transaction type\n",
    "fraud_by_type = df.groupby('type')['isFraud'].agg(['sum', 'count'])\n",
    "fraud_rate_by_type = (fraud_by_type['sum'] / fraud_by_type['count'] * 100).sort_values(ascending=False)\n",
    "\n",
    "axes[1,1].bar(fraud_rate_by_type.index, fraud_rate_by_type.values, color='coral')\n",
    "axes[1,1].set_title('Fraud Rate by Transaction Type (%)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].set_ylabel('Fraud Rate (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"  Key Insights:\")\n",
    "print(f\"• Highest fraud rate transaction type: {fraud_rate_by_type.index[0]} ({fraud_rate_by_type.iloc[0]:.2f}%)\")\n",
    "print(f\"• Average fraud transaction amount: ${df_fraud.mean():,.2f}\")\n",
    "print(f\"• Average legitimate transaction amount: ${df_legit.mean():,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8101e3f6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Preprocessing Script Development {#preprocessing}\n",
    "\n",
    "Let's create a comprehensive preprocessing module for our fraud detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab154a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create preprocessing.py script\n",
    "preprocessing_code = '''\"\"\"\n",
    "Data Preprocessing Module for Financial Fraud Detection\n",
    "\n",
    "This module contains functions for loading, cleaning, and preprocessing\n",
    "the financial transaction data from the Kaggle dataset.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for preprocessing financial transaction data for fraud detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoders = {}\n",
    "        self.feature_columns = []\n",
    "        \n",
    "    def load_data(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load data from CSV file.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading data from {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            logger.info(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handle missing values in the dataset.\"\"\"\n",
    "        logger.info(\"Handling missing values\")\n",
    "        \n",
    "        missing_summary = df.isnull().sum()\n",
    "        if missing_summary.sum() > 0:\n",
    "            logger.info(f\"Missing values found: {missing_summary[missing_summary > 0]}\")\n",
    "            \n",
    "            # Handle numerical columns\n",
    "            numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            for col in numerical_cols:\n",
    "                if df[col].isnull().any():\n",
    "                    df[col].fillna(df[col].median(), inplace=True)\n",
    "            \n",
    "            # Handle categorical columns\n",
    "            categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "            for col in categorical_cols:\n",
    "                if df[col].isnull().any():\n",
    "                    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "        \n",
    "        logger.info(\"Missing values handled successfully\")\n",
    "        return df\n",
    "    \n",
    "    def encode_categorical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Encode categorical features using label encoding.\"\"\"\n",
    "        logger.info(\"Encoding categorical features\")\n",
    "        \n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col not in ['isFraud', 'nameOrig', 'nameDest']:\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[col] = le.fit_transform(df[col].astype(str))\n",
    "                self.label_encoders[col] = le\n",
    "                logger.info(f\"Encoded column: {col}\")\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    def prepare_data(self, df: pd.DataFrame, target_column: str = 'isFraud', \n",
    "                    test_size: float = 0.2, random_state: int = 42) -> tuple:\n",
    "        \"\"\"Complete data preprocessing pipeline.\"\"\"\n",
    "        logger.info(\"Starting complete data preprocessing pipeline\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = self.handle_missing_values(df)\n",
    "        \n",
    "        # Encode categorical features\n",
    "        df = self.encode_categorical_features(df)\n",
    "        \n",
    "        # Remove high cardinality columns\n",
    "        columns_to_drop = ['nameOrig', 'nameDest']\n",
    "        existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns:\n",
    "            df = df.drop(columns=existing_columns)\n",
    "            logger.info(f\"Dropped high cardinality columns: {existing_columns}\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        if target_column in df.columns:\n",
    "            X = df.drop(columns=[target_column])\n",
    "            y = df[target_column]\n",
    "        else:\n",
    "            logger.warning(f\"Target column '{target_column}' not found.\")\n",
    "            X = df\n",
    "            y = None\n",
    "        \n",
    "        self.feature_columns = X.columns.tolist()\n",
    "        \n",
    "        # Split the data\n",
    "        if y is not None:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Data preprocessing completed. Training set size: {X_train.shape}\")\n",
    "            return X_train, X_test, y_train, y_test\n",
    "        else:\n",
    "            return X, None, None, None\n",
    "'''\n",
    "\n",
    "# Write preprocessing.py file\n",
    "preprocessing_path = project_root / \"src\" / \"preprocessing.py\"\n",
    "with open(preprocessing_path, 'w') as f:\n",
    "    f.write(preprocessing_code)\n",
    "\n",
    "print(\"✓ preprocessing.py created successfully!\")\n",
    "print(f\"  Location: {preprocessing_path}\")\n",
    "print(\"\\n  Key preprocessing features:\")\n",
    "print(\"  • Missing value handling\")\n",
    "print(\"  • Categorical encoding\")\n",
    "print(\"  • Data splitting with stratification\")\n",
    "print(\"  • Logging for monitoring\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
